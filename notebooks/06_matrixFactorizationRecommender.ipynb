{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = pd.read_csv('../data/cleaned/cleaned_user_rating.csv')\n",
    "rating_df_copy = rating_df.copy()\n",
    "movies_df = pd.read_csv('../data/cleaned/cleaned_movies_details.csv', usecols=['movie_id', 'title', 'genres', 'overview', 'director', 'stars', 'img_url'])\n",
    "\n",
    "# ID to number\n",
    "rating_df_copy['user_id_number'] = rating_df_copy['user_id'].astype('category').cat.codes.values\n",
    "rating_df_copy['movie_id_number'] = rating_df_copy['movie_id'].astype('category').cat.codes.values\n",
    "# rating_df_copy = rating_df_copy.groupby('movie_id') \\\n",
    "# .filter(lambda x : len(x) >= 30) # 15\n",
    "\n",
    "train_data = rating_df_copy[['user_id_number', 'movie_id_number', 'rating']].values\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['genres'] = movies_df['genres'].str.replace(',',' ')\n",
    "movies_df['genres'] = movies_df['genres'].str.replace('Sci-Fi','SciFi')\n",
    "movies_df['genres'] = movies_df['genres'].str.replace('Film-Noir','FilmNoir')\n",
    "movies_df['genres'] = movies_df['genres'].str.replace('Reality-TV','RealityTV')\n",
    "movies_df['genres'] = movies_df['genres'].str.replace('Talk-Show','TalkShow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "# Update content creation for semantic embeddings\n",
    "movies_df['content'] = movies_df.apply(\n",
    "    lambda x: f\"{x['title']} is a {x['genres']} movie directed by {x['director']} starring {x['stars']}. Overview: {x['overview']}\",\n",
    "    axis=1\n",
    ")\n",
    "# Create transformer embeddings\n",
    "transformer_embeddings = model.encode(movies_df['content'].tolist(), show_progress_bar=True)\n",
    "# Compute cosine similarity matrix\n",
    "sim_matrix = cosine_similarity(transformer_embeddings)\n",
    "np.save('./sim_matrix.npy', sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rating_df_copy)\n",
    "# Build the mappings\n",
    "number_to_user_id = dict(enumerate(rating_df_copy['user_id'].astype('category').cat.categories))\n",
    "user_id_to_number = {v: k for k, v in number_to_user_id.items()}\n",
    "number_to_movie_id = dict(enumerate(rating_df_copy['movie_id'].astype('category').cat.categories))\n",
    "movie_id_to_number = {v: k for k, v in number_to_movie_id.items()}\n",
    "# Example: Get user_id_number for a specific user_id\n",
    "user_id = 'ur127508339'  \n",
    "user_id_number = user_id_to_number[user_id]\n",
    "print(user_id_number)\n",
    "print(movie_id_to_number['tt7737800'])\n",
    "# Example: Get user_id for a specific user_id_number\n",
    "user_id_number = 35310  \n",
    "user_id = number_to_user_id[user_id_number]\n",
    "print(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movieURL(movie_id):\n",
    "    return movies_df[movies_df.movie_id == movie_id].img_url.values[0]\n",
    "\n",
    "print(get_movieURL('tt7737800'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ratings_by_user(train_data, random_state=420):\n",
    "    \"\"\"\n",
    "    Split ratings for each user into train, validation, and test sets\n",
    "    with 60:20:20 split and maintaining rating count distribution\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Unique users\n",
    "    users = np.unique(train_data[:, 0])\n",
    "    \n",
    "    # Initialize empty lists for split data\n",
    "    train_data_list = []\n",
    "    val_data_list = []\n",
    "    test_data_list = []\n",
    "    \n",
    "    for user in users:\n",
    "        user_ratings = train_data[train_data[:, 0] == user]\n",
    "        # Sort ratings in ascending order (assuming they are sorted by timestamp or order)\n",
    "        user_ratings = user_ratings[user_ratings[:, 2].argsort()]\n",
    "        total_ratings = len(user_ratings)\n",
    "        train_end = math.ceil(total_ratings * 0.6)\n",
    "        val_end = train_end + math.ceil(total_ratings * 0.2)\n",
    "        \n",
    "        # Split the data\n",
    "        train_data_list.append(user_ratings[:train_end])\n",
    "        val_data_list.append(user_ratings[train_end:val_end])\n",
    "        test_data_list.append(user_ratings[val_end:])\n",
    "    \n",
    "    # Concatenate the lists\n",
    "    train_data = np.vstack(train_data_list)\n",
    "    val_data = np.vstack(val_data_list)\n",
    "    test_data = np.vstack(test_data_list)\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, val_data, test_data = split_ratings_by_user(train_data)\n",
    "# np.savez_compressed('../cleaned_data/data.npz', train=train_data, val=val_data, test=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendation(cf_model, test_data, n_top=5):\n",
    "    hit_ratios, ndcg_scores = [], []\n",
    "    \n",
    "    for u in np.unique(test_data[:, 0]):\n",
    "        # Get movies with ratings 8-10 in test data\n",
    "        user_test_ratings = test_data[test_data[:, 0] == u]\n",
    "        high_rated_test_movies = user_test_ratings[user_test_ratings[:, 2] >= 8, 1]\n",
    "        \n",
    "        if len(high_rated_test_movies) != 0:\n",
    "            # Get all unrated movies\n",
    "            train_movies = cf_model.train_data[cf_model.train_data[:, 0] == u, 1]\n",
    "            all_movies = np.arange(cf_model.n_movies)\n",
    "            unrated_movies = all_movies[~np.isin(all_movies, np.concatenate([train_movies, user_test_ratings[:, 1]]))]\n",
    "            \n",
    "            # Combine the high-rated movie with 99 other unrated movies\n",
    "            sampled_unrated_movies = np.random.choice(unrated_movies, 99, replace=False)\n",
    "            \n",
    "            hit_ratio = 0\n",
    "            dcg = 0\n",
    "            # For each high-rated test movie\n",
    "            for high_rated_movie in high_rated_test_movies:\n",
    "                \n",
    "                candidate_movies = np.concatenate([[high_rated_movie], sampled_unrated_movies])\n",
    "                movie_scores = {i: cf_model.pred(u, i) for i in candidate_movies}\n",
    "                \n",
    "                # Sort movies by predicted score\n",
    "                sorted_movies = sorted(movie_scores, key=movie_scores.get, reverse=True)[:n_top]\n",
    "                \n",
    "                # Calculate the benchmark\n",
    "                if high_rated_movie in sorted_movies:\n",
    "                    hit_ratio += 1 \n",
    "                    rank = sorted_movies.index(high_rated_movie) + 1\n",
    "                    dcg += 1 / math.log2(rank + 1)  # Discount for rank\n",
    "            hit_ratios.append(hit_ratio / len(high_rated_test_movies))\n",
    "            ndcg_scores.append(dcg / len(high_rated_test_movies))\n",
    "            \n",
    "    return np.mean(hit_ratios), np.mean(ndcg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../checkpoints/data.npz')\n",
    "train_data = data['train']\n",
    "val_data = data['val']\n",
    "test_data = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISMF(object):\n",
    "    global user_id_to_number, number_to_user_id, movie_id_to_number, number_to_movie_id\n",
    "    def __init__(self, train_data, test_data, n_factors=10, learning_rate=0.01, n_epochs=10):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_users = int(np.max(self.train_data[:, 0])) + 1 # 1 because index from 0\n",
    "        self.n_movies = int(np.max(self.train_data[:, 1])) + 1 \n",
    "        # P, Q's size may be big at first to add new user/film\n",
    "        self.P = np.random.normal(scale=1.0 / self.n_factors, size=(self.n_users + 100, self.n_factors))\n",
    "        self.Q = np.random.normal(scale=1.0 / self.n_factors, size=(self.n_movies + 100, self.n_factors))\n",
    "    \n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def fit(self):\n",
    "        best_loss = float('inf')\n",
    "        no_improve_epochs = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(self.train_data)\n",
    "            # Use tqdm to create a progress bar for the inner loop\n",
    "            for u, i, r in tqdm(self.train_data, desc=f'Epoch {epoch + 1}/{self.n_epochs}', unit='rating'):\n",
    "                u, i = int(u), int(i)\n",
    "                pred = self.pred(u, i)\n",
    "                error = r - pred\n",
    "                # Update P and Q\n",
    "                self.P[u, :] += self.learning_rate * error * self.Q[i, :]\n",
    "                self.Q[i, :] += self.learning_rate * error * self.P[u, :]\n",
    "            \n",
    "            train_loss = self.loss(self.train_data)\n",
    "            test_loss = self.loss(self.test_data)\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "            print(f\"Test loss: {test_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if test_loss < best_loss + 1e-6:\n",
    "                best_loss = test_loss\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                if no_improve_epochs >= 5:\n",
    "                    print(\"Early stopping: No improvement for 5 consecutive epochs.\")\n",
    "                    break\n",
    "\n",
    "    def incremental_update(self, new_ratings):\n",
    "        # Convert new_ratings from IDs to numerical indices\n",
    "        processed_ratings = []\n",
    "        for user_id, movie_id, rating in new_ratings:\n",
    "            # Check and update user_id_to_number\n",
    "            if user_id not in user_id_to_number:\n",
    "                user_id_to_number[user_id] = self.n_users\n",
    "                number_to_user_id[self.n_users] = user_id\n",
    "                self.n_users += 1\n",
    "\n",
    "            # Check and update movie_id_to_number\n",
    "            if movie_id not in movie_id_to_number:\n",
    "                movie_id_to_number[movie_id] = self.n_movies\n",
    "                number_to_movie_id[self.n_movies] = movie_id\n",
    "                self.n_movies += 1\n",
    "\n",
    "            # Convert IDs to numbers and append to processed_ratings\n",
    "            u = user_id_to_number[user_id]\n",
    "            i = movie_id_to_number[movie_id]\n",
    "            processed_ratings.append([u, i, rating])\n",
    "\n",
    "        # Convert processed_ratings to a NumPy array\n",
    "        processed_ratings = np.array(processed_ratings)\n",
    "        # Update the train_data matrix with the new ratings\n",
    "        self.train_data = np.vstack((self.train_data, processed_ratings))\n",
    "        # Incremental learning using new ratings\n",
    "        for u, i, r in processed_ratings:\n",
    "            u, i, r = int(u), int(i), int(r)\n",
    "            pred = self.pred(u, i)\n",
    "            error = r - pred\n",
    "            # Update P and Q\n",
    "            self.P[u, :] += self.learning_rate * error * self.Q[i, :]\n",
    "            self.Q[i, :] += self.learning_rate * error * self.P[u, :]\n",
    "\n",
    "    def pred(self, u, i):\n",
    "        return self.P[u, :].dot(self.Q[i, :].T)\n",
    "            \n",
    "    def print_recommendation(self, user_id, number=10):\n",
    "        recommended_items = self.recommend(user_id_to_number[user_id])\n",
    "        recommended_items = recommended_items[:number]\n",
    "        \n",
    "        print(f'Recommended movie(s) for user {user_id} : {recommended_items}')\n",
    "            \n",
    "        cols = 5 if number > 5 else number\n",
    "        rows = math.ceil(number/cols)\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "        urls = []\n",
    "\n",
    "        for i in recommended_items:\n",
    "            movie_id = number_to_movie_id[i]\n",
    "            urls.append(get_movieURL(movie_id))\n",
    "            \n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < number:\n",
    "                ax.imshow(np.array(Image.open(urllib.request.urlopen(urls[i]))))\n",
    "                fig.tight_layout()\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "            \n",
    "    def recommend(self, u):        \n",
    "        \"\"\"\n",
    "        Determine all unrated items should be recommended for user u\n",
    "        \"\"\"\n",
    "        ids = np.where(self.train_data[:, 0] == u)[0]\n",
    "        items_rated_by_u = self.train_data[ids, 1].tolist()\n",
    "        recommended_items = {}\n",
    "        for i in range(self.n_movies):\n",
    "            if i not in items_rated_by_u:\n",
    "                recommended_items[i] = self.pred(u, i)\n",
    "                \n",
    "        # # Visualization of prediction values distribution\n",
    "        # pred_values = list(recommended_items.values())\n",
    "        \n",
    "        # # Create figure and axis\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # # Option 1: Histogram\n",
    "        # plt.hist(pred_values, bins=50, edgecolor='black')\n",
    "        # plt.title(f'Distribution of Predicted Ratings for User {u}')\n",
    "        # plt.xlabel('Predicted Rating')\n",
    "        # plt.ylabel('Frequency')\n",
    "        # plt.show()\n",
    "        \n",
    "        # # Option 2: Density plot using seaborn\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # sns.kdeplot(data=pred_values, fill=True)\n",
    "        # plt.title(f'Density Distribution of Predicted Ratings for User {u}')\n",
    "        # plt.xlabel('Predicted Rating')\n",
    "        # plt.ylabel('Density')\n",
    "        # plt.show()\n",
    "        \n",
    "        # # Optional: Print basic statistics\n",
    "        # print(f\"Mean prediction: {np.mean(pred_values):.2f}\")\n",
    "        # print(f\"Median prediction: {np.median(pred_values):.2f}\")\n",
    "        # print(f\"Standard deviation: {np.std(pred_values):.2f}\")\n",
    "        # print(f\"Min prediction: {min(pred_values):.2f}\")\n",
    "        # print(f\"Max prediction: {max(pred_values):.2f}\")\n",
    "\n",
    "        return sorted(recommended_items, key=recommended_items.get, reverse=True)\n",
    "    \n",
    "    def loss(self, data):\n",
    "        L = 0\n",
    "        for u, i, r in (data):\n",
    "            u, i = int(u), int(i)\n",
    "            pred = self.pred(u, i)\n",
    "            L += (r - pred)**2\n",
    "        L /= data.shape[0]\n",
    "        return math.sqrt(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf_model = ISMF(train_data, n_factors=2, learning_rate=1e-4, n_epochs=100)\n",
    "# cf_model.print_recommendation('ur127508339')\n",
    "# # Add new ratings incrementally\n",
    "# new_ratings = np.array([['ur127508339', 'tt0062292', 3], ['ur6969', 'tt6969', 7]])  # Example new ratings\n",
    "# cf_model.incremental_update(new_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "n_factors_list = [5, 10, 20, 50] # [5, 10, 20, 50]\n",
    "learning_rates = [1e-3, 1e-2, 5e-2, 1e-1] # [1e-3, 1e-2, 5e-2, 1e-1]\n",
    "\n",
    "# Open the CSV file and write the header once\n",
    "csv_file_path = 'results/ismf_results.csv'\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['n_factors', 'learning_rate', 'HR', 'NDCG']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Evaluate across all hyperparameter combinations\n",
    "for n_factors, learning_rate in product(n_factors_list, learning_rates):\n",
    "    # Initialize the model\n",
    "    cf_model = ISMF(train_data, val_data, n_factors=n_factors, learning_rate=learning_rate, n_epochs=100)\n",
    "    print(f\"ismf, n_factors: {n_factors}, learning_rate: {learning_rate}\")\n",
    "    \n",
    "    # Train the model\n",
    "    cf_model.fit()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    hit_ratio, ndcg = evaluate_recommendation(cf_model, test_data, n_top=5)\n",
    "    print(f\"Results - n_factors: {n_factors}, learning_rate: {learning_rate}, HR: {hit_ratio:.4f}, NDCG: {ndcg:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename = f\"../models/ismf/ismf{n_factors}_lr{learning_rate}.pkl\"\n",
    "    cf_model.save(model_filename)\n",
    "    \n",
    "    # Append the result to the CSV file\n",
    "    with open(csv_file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['n_factors', 'learning_rate', 'HR', 'NDCG'])\n",
    "        writer.writerow({\n",
    "            'n_factors': n_factors, \n",
    "            'learning_rate': learning_rate, \n",
    "            'HR': f\"{hit_ratio:.4f}\", \n",
    "            'NDCG': f\"{ndcg:.4f}\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RISMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RISMF(object):\n",
    "    global user_id_to_number, number_to_user_id, movie_id_to_number, number_to_movie_id\n",
    "    def __init__(self, train_data, test_data, n_factors=10, learning_rate=0.01, lambda_reg=0.1, n_epochs=10):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_users = int(np.max(self.train_data[:, 0])) + 1 # 1 because index from 0\n",
    "        self.n_movies = int(np.max(self.train_data[:, 1])) + 1\n",
    "         \n",
    "        # P, Q's size may be big at first to add new user/film\n",
    "        self.P = np.random.normal(scale=1.0 / self.n_factors, size=(self.n_users + 100, self.n_factors))\n",
    "        self.Q = np.random.normal(scale=1.0 / self.n_factors, size=(self.n_movies + 100, self.n_factors))\n",
    "    \n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def fit(self):\n",
    "        best_loss = float('inf')\n",
    "        no_improve_epochs = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(self.train_data)\n",
    "            # Use tqdm to create a progress bar for the inner loop\n",
    "            for u, i, r in tqdm(self.train_data, desc=f'Epoch {epoch + 1}/{self.n_epochs}', unit='rating'):\n",
    "                u, i = int(u), int(i)\n",
    "                pred = self.pred(u, i)\n",
    "                error = r - pred\n",
    "                # Update P and Q\n",
    "                self.P[u, :] += self.learning_rate * (error * self.Q[i, :] - self.lambda_reg * self.P[u, :])\n",
    "                self.Q[i, :] += self.learning_rate * (error * self.P[u, :] - self.lambda_reg * self.Q[i, :])\n",
    "\n",
    "            train_loss = self.loss(self.train_data)\n",
    "            test_loss = self.loss(self.test_data)\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "            print(f\"Test loss: {test_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if test_loss < best_loss + 1e-6:\n",
    "                best_loss = test_loss\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                if no_improve_epochs >= 5:\n",
    "                    print(\"Early stopping: No improvement for 5 consecutive epochs.\")\n",
    "                    break\n",
    "                \n",
    "    def incremental_update(self, new_ratings):\n",
    "        # Convert new_ratings from IDs to numerical indices\n",
    "        processed_ratings = []\n",
    "        for user_id, movie_id, rating in new_ratings:\n",
    "            # Check and update user_id_to_number\n",
    "            if user_id not in user_id_to_number:\n",
    "                user_id_to_number[user_id] = self.n_users\n",
    "                number_to_user_id[self.n_users] = user_id\n",
    "                self.n_users += 1\n",
    "\n",
    "            # Check and update movie_id_to_number\n",
    "            if movie_id not in movie_id_to_number:\n",
    "                movie_id_to_number[movie_id] = self.n_movies\n",
    "                number_to_movie_id[self.n_movies] = movie_id\n",
    "                self.n_movies += 1\n",
    "\n",
    "            # Convert IDs to numbers and append to processed_ratings\n",
    "            u = user_id_to_number[user_id]\n",
    "            i = movie_id_to_number[movie_id]\n",
    "            processed_ratings.append([u, i, rating])\n",
    "\n",
    "        # Convert processed_ratings to a NumPy array\n",
    "        processed_ratings = np.array(processed_ratings)\n",
    "        # Update the train_data matrix with the new ratings\n",
    "        self.train_data = np.vstack((self.train_data, processed_ratings))\n",
    "        # Incremental learning using new ratings\n",
    "        for u, i, r in processed_ratings:\n",
    "            u, i, r = int(u), int(i), int(r)\n",
    "            pred = self.pred(u, i)\n",
    "            error = r - pred\n",
    "            # Update P and Q\n",
    "            self.P[u, :] += self.learning_rate * (error * self.Q[i, :] - self.lambda_reg * self.P[u, :])\n",
    "            self.Q[i, :] += self.learning_rate * (error * self.P[u, :] - self.lambda_reg * self.Q[i, :])\n",
    "\n",
    "    def pred(self, u, i):\n",
    "        return self.P[u, :].dot(self.Q[i, :].T)\n",
    "            \n",
    "    def print_recommendation(self, user_id, number=10):\n",
    "        recommended_items = self.recommend(user_id_to_number[user_id])\n",
    "        recommended_items = recommended_items[:number]\n",
    "        \n",
    "        print(f'Recommended movie(s) for user {user_id} : {recommended_items}')\n",
    "            \n",
    "        cols = 5 if number > 5 else number\n",
    "        rows = math.ceil(number/cols)\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "        urls = []\n",
    "\n",
    "        for i in recommended_items:\n",
    "            movie_id = number_to_movie_id[i]\n",
    "            urls.append(get_movieURL(movie_id))\n",
    "            \n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < number:\n",
    "                ax.imshow(np.array(Image.open(urllib.request.urlopen(urls[i]))))\n",
    "                fig.tight_layout()\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "            \n",
    "    def recommend(self, u):        \n",
    "        \"\"\"\n",
    "        Determine all unrated items should be recommended for user u\n",
    "        \"\"\"\n",
    "        ids = np.where(self.train_data[:, 0] == u)[0]\n",
    "        items_rated_by_u = self.train_data[ids, 1].tolist()\n",
    "        recommended_items = {}\n",
    "        for i in range(self.n_movies):\n",
    "            if i not in items_rated_by_u:\n",
    "                recommended_items[i] = self.pred(u, i)\n",
    "\n",
    "        return sorted(recommended_items, key=recommended_items.get, reverse=True)\n",
    "    \n",
    "    def loss(self, data):\n",
    "        L = 0\n",
    "        for u, i, r in (data):\n",
    "            u, i = int(u), int(i)\n",
    "            pred = self.pred(u, i)\n",
    "            L += (r - pred)**2\n",
    "        L /= data.shape[0]\n",
    "        return math.sqrt(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "n_factors_list = [5, 10, 20, 50] # [5, 10, 20, 50]\n",
    "learning_rates = [1e-3, 1e-2, 5e-2, 1e-1] # [1e-3, 1e-2, 5e-2, 1e-1]\n",
    "\n",
    "# Open the CSV file and write the header once\n",
    "csv_file_path = 'results/rismf_results.csv'\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['n_factors', 'learning_rate', 'HR', 'NDCG']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Evaluate across all hyperparameter combinations\n",
    "for n_factors, learning_rate in product(n_factors_list, learning_rates):\n",
    "    # Initialize the model\n",
    "    cf_model = RISMF(train_data, val_data, n_factors=n_factors, learning_rate=learning_rate, lambda_reg=0.1, n_epochs=100)\n",
    "    print(f\"rismf, n_factors: {n_factors}, learning_rate: {learning_rate}\")\n",
    "    \n",
    "    # Train the model\n",
    "    cf_model.fit()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    hit_ratio, ndcg = evaluate_recommendation(cf_model, test_data, n_top=5)\n",
    "    print(f\"Results - n_factors: {n_factors}, learning_rate: {learning_rate}, HR: {hit_ratio:.4f}, NDCG: {ndcg:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename = f\"../checkpoints/rismf/rismf_nf{n_factors}_lr{learning_rate}.pkl\"\n",
    "    cf_model.save(model_filename)\n",
    "    \n",
    "    # Append the result to the CSV file\n",
    "    with open(csv_file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['n_factors', 'learning_rate', 'HR', 'NDCG'])\n",
    "        writer.writerow({\n",
    "            'n_factors': n_factors, \n",
    "            'learning_rate': learning_rate, \n",
    "            'HR': f\"{hit_ratio:.4f}\", \n",
    "            'NDCG': f\"{ndcg:.4f}\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf_model = load_model(\"../models/rismf_model_nf2_lr0.01.pkl\")\n",
    "# cf_model.print_recommendation('ur127508339')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRISMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRISMF(object):\n",
    "    global user_id_to_number, number_to_user_id, movie_id_to_number, number_to_movie_id\n",
    "    def __init__(self, train_data, test_data, n_factors=10, learning_rate=0.01, lambda_reg=0.1, n_epochs=10):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_users = int(np.max(self.train_data[:, 0])) + 1 # 1 because index from 0\n",
    "        self.n_movies = int(np.max(self.train_data[:, 1])) + 1\n",
    "         \n",
    "        # P, Q's size may be big at first to add new user/film\n",
    "        self.P = np.random.normal(scale=1.0 / self.n_factors, size=(self.n_users + 100, self.n_factors))\n",
    "        self.Q = np.random.normal(scale=1.0 / self.n_factors, size=(self.n_movies + 100, self.n_factors))\n",
    "        \n",
    "        # Fix P[1, :] and Q[2, :] to 1\n",
    "        self.P[1, :] = 1\n",
    "        self.Q[2, :] = 1\n",
    "    \n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def fit(self):\n",
    "        best_loss = float('inf')\n",
    "        no_improve_epochs = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(self.train_data)\n",
    "            # Use tqdm to create a progress bar for the inner loop\n",
    "            for u, i, r in tqdm(self.train_data, desc=f'Epoch {epoch + 1}/{self.n_epochs}', unit='rating'):\n",
    "                u, i = int(u), int(i)\n",
    "                pred = self.pred(u, i)\n",
    "                error = r - pred\n",
    "                \n",
    "                # Skip updates for fixed rows\n",
    "                if u != 1:\n",
    "                    self.P[u, :] += self.learning_rate * (error * self.Q[i, :] - self.lambda_reg * self.P[u, :])\n",
    "                if i != 2:\n",
    "                    self.Q[i, :] += self.learning_rate * (error * self.P[u, :] - self.lambda_reg * self.Q[i, :])\n",
    "            \n",
    "            train_loss = self.loss(self.train_data)\n",
    "            test_loss = self.loss(self.test_data)\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "            print(f\"Test loss: {test_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if test_loss < best_loss + 1e-6:\n",
    "                best_loss = test_loss\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                if no_improve_epochs >= 5:\n",
    "                    print(\"Early stopping: No improvement for 5 consecutive epochs.\")\n",
    "                    break\n",
    "                \n",
    "    def incremental_update(self, new_ratings):\n",
    "        # Convert new_ratings from IDs to numerical indices\n",
    "        processed_ratings = []\n",
    "        for user_id, movie_id, rating in new_ratings:\n",
    "            # Check and update user_id_to_number\n",
    "            if user_id not in user_id_to_number:\n",
    "                user_id_to_number[user_id] = self.n_users\n",
    "                number_to_user_id[self.n_users] = user_id\n",
    "                self.n_users += 1\n",
    "\n",
    "            # Check and update movie_id_to_number\n",
    "            if movie_id not in movie_id_to_number:\n",
    "                movie_id_to_number[movie_id] = self.n_movies\n",
    "                number_to_movie_id[self.n_movies] = movie_id\n",
    "                self.n_movies += 1\n",
    "\n",
    "            # Convert IDs to numbers and append to processed_ratings\n",
    "            u = user_id_to_number[user_id]\n",
    "            i = movie_id_to_number[movie_id]\n",
    "            processed_ratings.append([u, i, rating])\n",
    "\n",
    "        # Convert processed_ratings to a NumPy array\n",
    "        processed_ratings = np.array(processed_ratings)\n",
    "        # Update the train_data matrix with the new ratings\n",
    "        self.train_data = np.vstack((self.train_data, processed_ratings))\n",
    "        # Incremental learning using new ratings\n",
    "        for u, i, r in processed_ratings:\n",
    "            u, i, r = int(u), int(i), int(r)\n",
    "            pred = self.pred(u, i)\n",
    "            error = r - pred\n",
    "            \n",
    "            if u != 1:\n",
    "                self.P[u, :] += self.learning_rate * (error * self.Q[i, :] - self.lambda_reg * self.P[u, :])\n",
    "            if i != 2:\n",
    "                self.Q[i, :] += self.learning_rate * (error * self.P[u, :] - self.lambda_reg * self.Q[i, :])\n",
    "\n",
    "    def pred(self, u, i):\n",
    "        return self.P[u, :].dot(self.Q[i, :].T)\n",
    "            \n",
    "    def print_recommendation(self, user_id, number=10):\n",
    "        recommended_items = self.recommend(user_id_to_number[user_id])\n",
    "        recommended_items = recommended_items[:number]\n",
    "        \n",
    "        print(f'Recommended movie(s) for user {user_id} : {recommended_items}')\n",
    "            \n",
    "        cols = 5 if number > 5 else number\n",
    "        rows = math.ceil(number/cols)\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "        urls = []\n",
    "\n",
    "        for i in recommended_items:\n",
    "            movie_id = number_to_movie_id[i]\n",
    "            urls.append(get_movieURL(movie_id))\n",
    "            \n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < number:\n",
    "                ax.imshow(np.array(Image.open(urllib.request.urlopen(urls[i]))))\n",
    "                fig.tight_layout()\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "            \n",
    "    def recommend(self, u):        \n",
    "        \"\"\"\n",
    "        Determine all unrated items should be recommended for user u\n",
    "        \"\"\"\n",
    "        ids = np.where(self.train_data[:, 0] == u)[0]\n",
    "        items_rated_by_u = self.train_data[ids, 1].tolist()\n",
    "        recommended_items = {}\n",
    "        for i in range(self.n_movies):\n",
    "            if i not in items_rated_by_u:\n",
    "                recommended_items[i] = self.pred(u, i)\n",
    "        print(recommended_items)\n",
    "\n",
    "        return sorted(recommended_items, key=recommended_items.get, reverse=True)\n",
    "    \n",
    "    def loss(self, data):\n",
    "        L = 0\n",
    "        for u, i, r in (data):\n",
    "            u, i = int(u), int(i)\n",
    "            pred = self.pred(u, i)\n",
    "            L += (r - pred)**2\n",
    "        L /= data.shape[0]\n",
    "        return math.sqrt(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "n_factors_list = [5, 10, 20, 50] # [5, 10, 20, 50]\n",
    "learning_rates = [1e-3, 1e-2, 5e-2, 1e-1] # [1e-3, 1e-2, 5e-2, 1e-1]\n",
    "\n",
    "# Open the CSV file and write the header once\n",
    "csv_file_path = 'results/brismf_results.csv'\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['n_factors', 'learning_rate', 'HR', 'NDCG']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Evaluate across all hyperparameter combinations\n",
    "for n_factors, learning_rate in product(n_factors_list, learning_rates):\n",
    "    # Initialize the model\n",
    "    cf_model = BRISMF(train_data, val_data, n_factors=n_factors, learning_rate=learning_rate, lambda_reg=0.1, n_epochs=100)\n",
    "    print(f\"brismf, n_factors: {n_factors}, learning_rate: {learning_rate}\")\n",
    "    \n",
    "    # Train the model\n",
    "    cf_model.fit()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    hit_ratio, ndcg = evaluate_recommendation(cf_model, test_data, n_top=5)\n",
    "    print(f\"Results - n_factors: {n_factors}, learning_rate: {learning_rate}, HR: {hit_ratio:.4f}, NDCG: {ndcg:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename = f\"../checkpoints/brismf/brismf_nf{n_factors}_lr{learning_rate}.pkl\"\n",
    "    cf_model.save(model_filename)\n",
    "    \n",
    "    # Append the result to the CSV file\n",
    "    with open(csv_file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['n_factors', 'learning_rate', 'HR', 'NDCG'])\n",
    "        writer.writerow({\n",
    "            'n_factors': n_factors, \n",
    "            'learning_rate': learning_rate, \n",
    "            'HR': f\"{hit_ratio:.4f}\", \n",
    "            'NDCG': f\"{ndcg:.4f}\"\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
